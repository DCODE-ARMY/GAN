{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": " darshan_mainFrame.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "background_execution": "on",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DCODE-ARMY/GAN/blob/3DCNN/darshan_mainFrame_CNN-RNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3thPIvOhTAZj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dd5f22c9-5b58-4927-c427-be5d1fde4327"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tue Aug  2 16:03:43 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla V100-SXM2...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   37C    P0    25W / 300W |      0MiB / 16160MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hFgBae2ZBZ1Z"
      },
      "source": [
        ""
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nO6Vve8KsGpT"
      },
      "source": [
        "import os\n",
        "import tensorflow as tf\n",
        "import keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Flatten, Conv3D, MaxPooling3D, Dropout, BatchNormalization\n",
        "from keras import layers\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import h5py\n",
        "import pickle\n",
        "import cv2\n",
        "import collections\n",
        "from sklearn.utils import shuffle\n"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "image_size=256\n",
        "train_data_dir='/content/drive/MyDrive/crime detection/train'\n",
        "categories=['arrest', 'arson', 'abuse', 'normal', 'assault']\n",
        "test_data_dir='/content/drive/MyDrive/crime detection/test'\n",
        "batch_size=32\n",
        "temporal_length=30\n",
        "stride=15\n",
        "channel_axis=temporal_length\n",
        "repeat=1500\n",
        "feature_length=2048\n"
      ],
      "metadata": {
        "id": "iv_ncKw5BYuW"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YF8pgRw__4lO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f1f2dc9d-82ed-44af-f910-3be069244cc5"
      },
      "source": [
        "#getting list of videos\n",
        "import os \n",
        "global files_list\n",
        "temp_files_list=[]\n",
        "\n",
        "\n",
        "\n",
        "for i in categories:\n",
        "  temp_files_list.append(os.listdir(train_data_dir+'/{}'.format(i)))\n",
        "\n",
        "print([len(temp_files_list[i]) for i in range(5)])\n",
        "\n",
        "#changing files names to directory name\n",
        "\n",
        "\n",
        "for i in range(len(categories)):\n",
        "\n",
        "  for k in range(len(temp_files_list[i])):\n",
        "    temp_files_list[i][k] =train_data_dir+\"/{}/{}\".format(categories[i],temp_files_list[i][k])\n",
        "\n",
        "#len of all videos \n",
        "print([len(temp_files_list[i]) for i in range(5)])\n",
        "\n",
        "#changing categories to single list\n",
        "files_list=[]\n",
        "\n",
        "for i in range(len(temp_files_list)):\n",
        "  for j in range(len(temp_files_list[i])):\n",
        "    files_list.append(temp_files_list[i][j])    \n",
        "\n",
        "files_list=shuffle(files_list)\n",
        "len(files_list)\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[42, 36, 27, 41, 31]\n",
            "[42, 36, 27, 41, 31]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "177"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fSG0dLmxdcGD"
      },
      "source": [
        "import os\n",
        "\n",
        "\n",
        "# getting test videos name\n",
        "test_files=[]\n",
        "for i in os.listdir(test_data_dir):\n",
        "  test_files.append(i)\n",
        "\n",
        "# getting test video path \n",
        "for i in range(len(test_files)):\n",
        "  test_files[i]=test_data_dir+\"/{}\".format(test_files[i])\n",
        "\n",
        "# print(len(test_files))\n",
        "# test_files\n",
        "\n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_feature_extractor():\n",
        "    feature_extractor = tf.keras.applications.InceptionV3(\n",
        "        weights=\"imagenet\",\n",
        "        include_top=False,\n",
        "        pooling=\"avg\",\n",
        "        input_shape=(image_size, image_size, 3),\n",
        "    )\n",
        "    inputs = keras.Input((image_size, image_size, 3))\n",
        "    preprocess_input = keras.applications.inception_v3.preprocess_input(inputs)\n",
        "    \n",
        "    outputs = feature_extractor(preprocess_input)\n",
        "    return keras.Model(inputs, outputs, name=\"feature_extractor\")\n",
        "\n",
        "\n",
        "feature_extractor = build_feature_extractor()"
      ],
      "metadata": {
        "id": "tmCAzVqEdyBC"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T_Qo8l5BSthl"
      },
      "source": [
        "\n",
        "\"\"\"manualy shuffling data for epoch since using custom generator and also the bulit in gen wont shuffle first.\n",
        "it splits the data first and then at the end of every epoch it shuffles\"\"\"\n",
        "\n",
        "\n",
        "def gen(data=files_list,temporal_length=temporal_length,temporal_stride=stride,batch_size=batch_size):  \n",
        "  \n",
        "  batch_count=0\n",
        "  ch=0 #check var\n",
        "  frame_features=[]\n",
        "  frame_masks = []\n",
        "  labels=[]\n",
        "  chunk_frames=[]\n",
        "\n",
        "  for i in range(len(data)):\n",
        "    frame_count=0 #to check enough frames are available to append in queue\n",
        "    chunk_frames.clear() # batch list \n",
        "    frame_features.clear()\n",
        "\n",
        "    labels.clear()\n",
        "    batch_count=0 #when new video starts the batch begins form Zero\n",
        "  \n",
        "\n",
        "   \n",
        "    \n",
        "    cap=cv2.VideoCapture(data[i])\n",
        "    total_frames=int(cap.get(7))\n",
        "\n",
        "    for k in range(total_frames):\n",
        "      \n",
        "      if total_frames-frame_count >=temporal_length:\n",
        "\n",
        "        if len(frame_features) != temporal_length:\n",
        "          img=cap.read()[1]\n",
        "          img=cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "          \n",
        "          img=img.astype(np.float32)\n",
        "          img=tf.keras.layers.Resizing(height=256,width=256,interpolation='gaussian')(img)\n",
        "        \n",
        "         \n",
        "          frame_features.append(feature_extractor.predict(np.expand_dims(img,axis=0)).squeeze())\n",
        "        \n",
        "          frame_count+=1\n",
        "\n",
        "        if len(frame_features) == temporal_length:  \n",
        "\n",
        "          chunk_frames.append(np.array(frame_features))\n",
        "          labels.append(float(categories.index(data[i].split('_')[0].split('/')[-1])))\n",
        "          \n",
        "          \n",
        "          batch_count+=1\n",
        "                  \n",
        "            \n",
        "        if batch_count == batch_size:\n",
        "          chunk_frames=shuffle(chunk_frames)  \n",
        "          batch_return=np.array(chunk_frames)\n",
        "         \n",
        "          # labels_return=np.reshape(np.array(labels,ndmin=2),(32,1))     \n",
        "          labels_return=labels\n",
        "          frame_masks=np.ones(shape=(batch_size,temporal_length),dtype='bool')  \n",
        "                  \n",
        "          yield  (batch_return,frame_masks) , labels_return\n",
        "          \n",
        "          batch_count=0\n",
        "          labels.clear()\n",
        "          chunk_frames.clear()\n",
        "          frame_features.clear()\n",
        "          \n",
        "\n",
        "          batch_return=0\n",
        "          labels_return=0\n",
        "\n",
        "\n",
        "      else:\n",
        "        batch_count=0\n",
        "        continue\n",
        "    cap.release()\n",
        "\n",
        "\n",
        "gen_data=tf.data.Dataset.from_generator(gen, output_signature=( (tf.TensorSpec(shape=(batch_size, temporal_length,feature_length),dtype=tf.dtypes.bool),tf.TensorSpec(shape=(batch_size, temporal_length),dtype=tf.dtypes.int8)),tf.TensorSpec(shape=(batch_size),dtype=tf.dtypes.float32) ))\n",
        "gen_data=gen_data.prefetch(buffer_size=tf.data.AUTOTUNE).repeat(repeat)\n"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zFqdcqauMl0z"
      },
      "source": [
        "# #  np.isinf([np.inf, -np.inf, 1.0, np.nan])\n",
        "# import numpy as np\n",
        "# print(np.any(np.sum(np.array([7, 5, 1.0, np.nan]))))\n",
        "# np.isnan(np.sum(np.array([7, 5, 1.0, np.nan])))"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VkHTF00Qiwk5"
      },
      "source": [
        "def val_gen(val_data=test_files,temporal_lenght=temporal_length,temporal_stride=stride,batch_size=batch_size):  \n",
        "  chunk_frames=[]\n",
        "  batch_count=0\n",
        "  frame_features=[]\n",
        "  frame_masks = []\n",
        "  labels=[]\n",
        "  chunk_frames=[]\n",
        " \n",
        "\n",
        "  # val_data=shuffle(val_data)\n",
        "\n",
        "  for i in range(len(val_data)):\n",
        "\n",
        "\n",
        "    frame_count=0 #to check enough frames are available to append in queue\n",
        "    chunk_frames.clear() # batch list \n",
        "    \n",
        "    frame_features.clear()\n",
        "\n",
        "    labels.clear()\n",
        "    batch_count=0 #when new video starts the batch begins form Zero\n",
        "    temp_frames=[]\n",
        "    main_frames=[]\n",
        "    \n",
        "    \n",
        "\n",
        "    total_frames=int(cv2.VideoCapture(val_data[i]).get(7))\n",
        "    cap=cv2.VideoCapture(val_data[i])\n",
        "\n",
        "    for k in range(total_frames):\n",
        "      \n",
        "      \n",
        "      if total_frames-frame_count >=temporal_lenght:\n",
        "\n",
        "        if len(frame_features) != temporal_lenght:\n",
        "          img=cap.read()[1]\n",
        "          img=cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "          img=img.astype(np.float32)\n",
        "\n",
        "          img=tf.keras.layers.Resizing(image_size,image_size,interpolation='gaussian',crop_to_aspect_ratio=False)(img)\n",
        "          # img = tf.keras.layers.Rescaling(scale=1./255)(img)\n",
        "          frame_features.append(feature_extractor.predict(np.expand_dims(img,axis=0)).squeeze())\n",
        "          frame_count+=1\n",
        "\n",
        "        if len(frame_features) == temporal_lenght:\n",
        "\n",
        "          chunk_frames.append(np.array(frame_features))\n",
        "          labels.append(float(categories.index(val_data[i].split('_')[0].split('/')[-1])))\n",
        "          batch_count+=1\n",
        "            \n",
        "        if batch_count == batch_size:\n",
        "          chunk_frames=shuffle(chunk_frames)  \n",
        "          batch_return=np.array(chunk_frames)\n",
        "         \n",
        "          # labels_return=np.reshape(np.array(labels,ndmin=2),(32,1))     \n",
        "          labels_return=labels\n",
        "          frame_masks=np.ones(shape=(batch_size,temporal_length),dtype='bool')\n",
        "          \n",
        "\n",
        "          yield  (batch_return,frame_masks) , labels_return\n",
        "        \n",
        "          batch_count=0\n",
        "          labels.clear()\n",
        "          chunk_frames.clear()\n",
        "          frame_features.clear()\n",
        "\n",
        "          batch_return=0\n",
        "          labels_return=0\n",
        "\n",
        "\n",
        "\n",
        "      else:\n",
        "        continue\n",
        "    cap.release()\n",
        "\n",
        "val_data=tf.data.Dataset.from_generator(val_gen, output_signature=((tf.TensorSpec(shape=(batch_size, temporal_length, feature_length),dtype=tf.dtypes.float32),tf.TensorSpec(shape=(batch_size, temporal_length),dtype=tf.dtypes.bool)),tf.TensorSpec(shape=(batch_size),dtype=tf.dtypes.float32) ))\n",
        "val_data=val_data.prefetch(buffer_size=tf.data.AUTOTUNE).repeat(repeat)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_sequence_model():\n",
        "\n",
        "  frame_features_input = keras.Input((temporal_length, feature_length))\n",
        "  mask_input = keras.Input((temporal_length,), dtype=\"bool\")\n",
        "\n",
        "  # Refer to the following tutorial to understand the significance of using `mask`:\n",
        "  # https://keras.io/api/layers/recurrent_layers/gru/\n",
        "  x = keras.layers.GRU(64, return_sequences=True)(\n",
        "      frame_features_input, mask=mask_input\n",
        "  )\n",
        "  x = keras.layers.GRU(32)(x)\n",
        "  x = keras.layers.Dropout(0.4)(x)\n",
        "  x = keras.layers.Dense(512, activation=\"relu\")(x)\n",
        "  x=keras.layers.BatchNormalization()(x)\n",
        "  output = keras.layers.Dense(len(categories), activation=\"softmax\")(x)\n",
        "\n",
        "  rnn_model = keras.Model([frame_features_input, mask_input], output)\n",
        "\n",
        "  \n",
        "  return rnn_model\n",
        "\n"
      ],
      "metadata": {
        "id": "E0v6BEBK-k4W"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "label_processor = keras.layers.StringLookup(num_oov_indices=0, vocabulary=np.unique(['a','b','c','v','a','b']))\n",
        "print(label_processor.get_vocabulary())\n",
        "print(np.array(label_processor(['a','b']).numpy()).shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rQEcZ43WKwWJ",
        "outputId": "f43f962a-dba6-469c-dbf3-b6373628aa24"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['a', 'b', 'c', 'v']\n",
            "(2,)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/numpy/core/numeric.py:2446: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
            "  return bool(asarray(a1 == a2).all())\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "filepath='/content/drive/MyDrive/crime detection'\n",
        "checkpoint = keras.callbacks.ModelCheckpoint(filepath, save_weights_only=True, save_best_only=True, verbose=1)\n",
        "\n",
        "\n",
        "%tensorflow_version 2.x\n",
        "import tensorflow as tf\n",
        "device_name = tf.test.gpu_device_name()\n",
        "# for running on gpu\n",
        "if device_name != '/device:GPU:0':\n",
        "  raise SystemError('GPU device not found')\n",
        "print('Found GPU at: {}'.format(device_name))\n",
        "\n",
        "\n",
        "class CustomCallback(keras.callbacks.Callback):\n",
        "\n",
        "  def on_epoch_end(self, epoch, logs=None):\n",
        "    global files_list\n",
        "    files_list=shuffle(files_list)\n",
        "\n",
        "csv_logger=tf.keras.callbacks.CSVLogger('/content/drive/MyDrive/crime detection/log.csv', separator=\",\", append=True)      \n",
        "    \n",
        "\n",
        "# initial_learning_rate = 0.01\n",
        "# lr_schedule = keras.optimizers.schedules.ExponentialDecay(\n",
        "#     initial_learning_rate, decay_steps=1000, decay_rate=0.96, staircase=False\n",
        "# )\n",
        "\n",
        "# lr_scheduler=tf.keras.callbacks.LearningRateScheduler(lr_schedule)\n",
        "# Define callbacks.\n",
        "checkpoint_cb = keras.callbacks.ModelCheckpoint(\n",
        "    \"/content/drive/MyDrive/crime detection/model.{epoch:02d}-{val_accuracy:.2f}.h5\", save_best_only=True,monitor='val_accuracy'\n",
        ")\n",
        "\n",
        "adam=tf.keras.optimizers.Adam(learning_rate=0.0002, beta_1=0.9, beta_2=0.999, epsilon=1e-07)\n",
        "\n",
        "\n",
        "\n",
        "with tf.device('/device:GPU:0'):\n",
        "\n",
        "  model=get_sequence_model()\n",
        "  model.compile(\n",
        "      \n",
        "      loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
        "      \n",
        "      optimizer=adam,\n",
        "\n",
        "      metrics=[\"accuracy\"]  )\n",
        "  \n",
        "model.fit(gen_data,validation_data=val_data,validation_steps=48,epochs=1000,steps_per_epoch=100,callbacks=[csv_logger,checkpoint_cb,CustomCallback()],batch_size=batch_size)\n",
        "  \n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z9kYmU98-knA",
        "outputId": "45b87af5-a0e7-4864-a503-9355779d5f14"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found GPU at: /device:GPU:0\n",
            "Epoch 1/1000\n",
            "150/150 [==============================] - 474s 3s/step - loss: 1.2238 - accuracy: 0.5529 - val_loss: 1.1094 - val_accuracy: 0.0000e+00\n",
            "Epoch 2/1000\n",
            "150/150 [==============================] - 536s 4s/step - loss: 1.4534 - accuracy: 0.4792 - val_loss: 0.4180 - val_accuracy: 1.0000\n",
            "Epoch 3/1000\n",
            "150/150 [==============================] - 512s 3s/step - loss: 1.1653 - accuracy: 0.6077 - val_loss: 4.6329 - val_accuracy: 0.0000e+00\n",
            "Epoch 4/1000\n",
            " 55/150 [==========>...................] - ETA: 5:29 - loss: 1.2231 - accuracy: 0.6182"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "ThZwmYrF-kJ-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xYPCQLkNUmUM"
      },
      "source": [
        "#678  train batches\n",
        "#49 test batches"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dr1GzHgJq4x7"
      },
      "source": [
        "# x_=[]\n",
        "# for j in  test_files:\n",
        "#   x_.append(cv2.VideoCapture(j).get(7))\n",
        "#   # print(j)\n",
        "# print(x_)\n",
        "# print(sum(x_))\n",
        "# print((int(sum(x_))-30)/15,((int(sum(x_))-30)/15)/32)\n",
        "\n",
        "# 23848.0 #total frames\n",
        "# 1587.8666666666666 #after made into chunks\n",
        "# 49.62083333333333 #after put into batches. So totally 49 batches available"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3S4gUlZCpbKZ"
      },
      "source": [
        "# x_=[]\n",
        "# for i in range(5):\n",
        "#   for j in range(len(files_list[i])):\n",
        "#     x_.append(cv2.VideoCapture(files_list[i][j]).get(7))\n",
        "# print(x_)\n",
        "# print(sum(x_))\n",
        "# print((int(sum(x_))-30)/15,((int(sum(x_))-30)/15)/32)\n",
        "\n",
        "# (11275-30)/15\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xok69s-UsP8m"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W92I3vsjSWfg"
      },
      "source": [
        "# 256%9"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "jLcS4MXuzBcO"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LbbTWLpSTKLs"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u9L9-ynZGM-m"
      },
      "source": [
        "# from tensorflow.keras.utils import plot_model\n",
        "\n",
        "# plot_model(model, to_file='convolutional_neural_network.png')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rvKlPCQkaUH7"
      },
      "source": [
        "# for i in range(5):\n",
        "#   print(files_list[i][0])\n",
        "#   files_list[i]=shuffle(files_list[i])\n",
        "#   print(files_list[i][0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3%3 ==0\n"
      ],
      "metadata": {
        "id": "rSrRzke_pG6T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "XbG08eVJICpd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "w0LHt-BcIChW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "s8GYI_LPICXP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zToojgzOFLpt"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2YWtVUm3c3I3"
      },
      "source": [
        "4279/32  #valid 133\n",
        "\n",
        "1163 # train\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uiDgP_cwcRb9"
      },
      "source": [
        "# loss:2.0897 acc:0.2936 val_loss: 4.4075 - val_accuracy: 0.4333 epoch 3\n",
        "# loss: 1.9303 - accuracy: 0.2906 - val_loss: 8.7073 - val_accuracy: 0.4333 epoch 4"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "avVPP_yh3IHy"
      },
      "source": [
        "import cv2\n",
        "from google.colab.patches import cv2_imshow\n",
        "import numpy as np\n",
        "\n",
        "total_frames=int(cv2.VideoCapture('/content/drive/MyDrive/train/abuse/abuse_8.mp4').get(7))\n",
        "print(total_frames)\n",
        "tol=0\n",
        "\n",
        "# img is 2D image data\n",
        "# tol  is tolerance\n",
        "cap=cv2.VideoCapture('/content/drive/MyDrive/train/abuse/abuse_8.mp4')\n",
        "img=cap.read()[1]\n",
        "# img=cv2.resize(img,(180,180))\n",
        "cv2_imshow(img)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o3TR9xfR5W4Z"
      },
      "source": [
        "y_nonzero, x_nonzero, _ = np.nonzero(im)\n",
        "im=im[np.min(y_nonzero):np.max(y_nonzero), np.min(x_nonzero):np.max(x_nonzero)]\n",
        "cv2_imshow(im)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TUss70t5cVK_"
      },
      "source": [
        "# to try \n",
        "\"\"\"\n",
        "Clip the gradients to prevent their explosion. For instance in Keras you could use clipnorm=1. or clipvalue=1. as parameters for your optimizer.\n",
        "\n",
        "Try to increase the batch size (e.g. 32 to 64 or 128) to increase the stability of your optimization.\n",
        "\n",
        "Check the size of your last batch which may be different from the batch size.\n",
        "\n",
        "Try to increase the batch size (e.g. 32 to 64 or 128) to increase the stability of your optimization.\n",
        "\n",
        "Check validity of inputs (no NaNs or sometimes 0s). i.e df.isnull().any()\n",
        "array_sum = np.sum(array)\n",
        "array_has_nan = np.isnan(array_sum)\n",
        "\"\"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8BG662yFa4tt"
      },
      "source": [
        "\"\"\"Add regularization to add l1 or l2 penalties to the weights. Otherwise, try a smaller l2 reg. i.e l2(0.001), \n",
        "      or remove it if already exists.\n",
        "\n",
        "Try a smaller Dropout rate.\n",
        "\n",
        "Clip the gradients to prevent their explosion. For instance in Keras you could use clipnorm=1. or clipvalue=1. as parameters for your optimizer.\n",
        "\n",
        "Check validity of inputs (no NaNs or sometimes 0s). i.e df.isnull().any()\n",
        "\n",
        "Replace optimizer with Adam which is easier to handle. Sometimes also replacing sgd with rmsprop would help.\n",
        "\n",
        "Use RMSProp with heavy regularization to prevent gradient explosion.\n",
        "\n",
        "Try normalizing your data, or inspect your normalization process for any bad values introduced.\n",
        "\n",
        "Verify that you are using the right activation function (e.g. using a softmax instead of sigmoid for multiple class classification).\n",
        "\n",
        "Try to increase the batch size (e.g. 32 to 64 or 128) to increase the stability of your optimization.\n",
        "\n",
        "Check the size of your last batch which may be different from the batch size.\"\"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kq4FAg6WJKSR"
      },
      "source": [
        "# # model.history.history\n",
        "# import pickle\n",
        "# # with open('/content/drive/MyDrive/history/trainHistoryDict_2', 'wb') as file_pi:\n",
        "# #   pickle.dump(model.history.history, file_pi)\n",
        "# history = pickle.load(open('/content/drive/MyDrive/history/trainHistoryDict_main', \"rb\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hDPTD9oZqRPR"
      },
      "source": [
        "# aaaa=history\n",
        "# bb=history\n",
        "\n",
        "# for d in aaaa.keys():\n",
        "#   bb[d].append(aaaa[d][-1])\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TOMd67_r7QX7"
      },
      "source": [
        "# model=tf.keras.models.load_model('/content/drive/MyDrive/re_train/my_model')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9rElP3RHGi9G"
      },
      "source": [
        "# model.save('/content/drive/MyDrive/re_train/my_model')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3wlJCwS9LBrg"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.plot(model.history.history['accuracy'])\n",
        "plt.plot(model.history.history['val_accuracy'])\n",
        "plt.title('model accuracy')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'test'], loc='upper left')\n",
        "plt.show()\n",
        "# summarize history for loss\n",
        "plt.plot(model.history.history['loss'])\n",
        "plt.plot(model.history.history['val_loss'])\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'test'], loc='upper left')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kL9GnCBII16L"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.plot(model.history.history['accuracy'])\n",
        "plt.plot(model.history.history['val_accuracy'])\n",
        "plt.title('model accuracy')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'test'], loc='upper left')\n",
        "plt.show()\n",
        "# summarize history for loss\n",
        "plt.plot(model.history.history['loss'])\n",
        "plt.plot(model.history.history['val_loss'])\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'test'], loc='upper left')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RmvT2ktN2KCI"
      },
      "source": [
        "#for predicting output\n",
        "\n",
        "import collections\n",
        "import copy\n",
        "import numpy as np\n",
        "import cv2\n",
        "from sklearn.utils import shuffle\n",
        "from keras.utils import np_utils\n",
        "import tensorflow as tf\n",
        "\n",
        "def predict(data,temporal_lenght=30,temporal_stride=20,batch_size=32):  \n",
        "  \n",
        "  chunk_frames=[]\n",
        "  batch_count=0\n",
        "  queue=collections.deque()\n",
        "  label=collections.deque()\n",
        "  # model=tf.keras.models.load_model('/content/drive/MyDrive/saved_model/my_model')\n",
        "\n",
        "  for i in range(len(data)):\n",
        "    frame_count=0 #to check enough frames are available to append in queue\n",
        "    chunk_frames.clear() # batch list \n",
        "    queue.clear() # frame queue\n",
        "    label.clear() # lable queue\n",
        "    batch_count=0 #when new video starts the batch begins form Zero\n",
        "    lable_list=[0,0,0,0,0]#for labeling \n",
        "    \n",
        "    total_frames=int(cv2.VideoCapture(data[i]).get(7))\n",
        "    print(total_frames)\n",
        "    cap=cv2.VideoCapture(data[i])\n",
        "    for k in range(total_frames):\n",
        "      \n",
        "      if total_frames-frame_count >=temporal_lenght:\n",
        "        if len(queue) != temporal_lenght:\n",
        "          queue.append(cv2.resize(cap.read()[1],(180,180)))\n",
        "          #queue.append(cap.read()[1])\n",
        "          frame_count+=1\n",
        "\n",
        "        if len(queue) == temporal_lenght:\n",
        "          \n",
        "          chunk_frames.append(np.array(queue,ndmin=4))\n",
        "          for stride in range(temporal_stride):\n",
        "            queue.popleft()          \n",
        "          batch_return=np.array(chunk_frames)\n",
        "\n",
        "          #print(model.predict(batch_return))\n",
        "                          \n",
        "          yield  batch_return \n",
        "        \n",
        "        chunk_frames.clear()\n",
        "        batch_return=0\n",
        "        \n",
        "\n",
        "\n",
        "\n",
        "    else:\n",
        "      batch_count=0\n",
        "      continue\n",
        "    cap.release()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sEhlASJY9p4c"
      },
      "source": [
        "# 'arrest'-0, 'abuse'-1, 'Normal_Videos'-2, 'Arson '-3, 'assault'-4\n",
        "'Normal_Videos', 'assault', 'arson', ' arrest', 'abuse'\n",
        "'Normal_Videos', 'assault', 'arson', ' arrest', 'abuse'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QHTSN-l3aSIi"
      },
      "source": [
        "model=tf.keras.models.load_model('/content/drive/MyDrive/high acc/model.11-0.71.h5')\n",
        "model.compile()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w7zo8Nuoaf3j"
      },
      "source": [
        "model.save('/content/drive/MyDrive/new_model')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IomjqVPIqnFt"
      },
      "source": [
        "# to predict output\n",
        "\n",
        "\n",
        "dataa=['/content/drive/MyDrive/train/Normal_Videos/Normal_Videos_015_x264.mp4']\n",
        "pr=predict(data=dataa)\n",
        "ff=[]\n",
        "for i in range(1000):\n",
        "  x=next(pr)\n",
        "  ff.append(model.predict(x))\n",
        "  print(model.predict(x))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e-lUpml2Bcr0"
      },
      "source": [
        "# print(np.count(np.array(ff),3))\n",
        "\n",
        "unique, counts = np.unique(np.array(ff), return_counts=True)\n",
        "dict(zip(unique, counts))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UUS_SRrdSFKK"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "loss_train = model.history.history['loss']\n",
        "loss_val = model.history.history['val_loss']\n",
        "epochs = range(1,3,1)\n",
        "plt.plot(epochs, loss_train, 'g', label='Training loss')\n",
        "plt.plot(epochs, loss_val, 'b', label='validation loss')\n",
        "plt.title('Training and Validation loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wgF1rO0cwR7D"
      },
      "source": [
        "import cv2\n",
        "\n",
        "from google.colab.patches import cv2_imshow\n",
        "face=cv2.imread(\"/content/im_1.jfif\")\n",
        "cv2_imshow(face)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R1Z8AkaFTP01"
      },
      "source": [
        "import tensorflow_hub as hub\n",
        "# import tensorflow as tf\n",
        "\n",
        "# model = hub.load(\"https://tfhub.dev/captain-pool/esrgan-tf2/1\")\n",
        "# # To add an extra dimension for batch, use tf.expand_dims()\n",
        "# low_resolution_image = tf.expand_dims(face, 0) # Low Resolution Image of shape [batch_size, height, width, 3]\n",
        "# low_resolution_image = tf.cast(low_resolution_image, tf.float32)\n",
        "# super_resolution = model(low_resolution_image) # Perform Super Resolution here\n",
        "# print(super_resolution[0].shape)\n",
        "\n",
        "# cv2_imshow(super_resolution[0].numpy())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yQ7Q2OLqv8OA"
      },
      "source": [
        "! pip install git+https://github.com/rcmalli/keras-vggface.git\n",
        "\n",
        "!pip show keras-vggface\n",
        "\n",
        "!pip install keras_applications\n",
        "# check version of keras_vggface\n",
        "import keras_vggface\n",
        "# print version\n",
        "print(keras_vggface.__version__)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "188nCWA2wD3I"
      },
      "source": [
        "!pip install mtcnn\n",
        "# confirm mtcnn was installed correctly\n",
        "import mtcnn\n",
        "# print version\n",
        "print(mtcnn.__version__)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w78nxSHRwHpV"
      },
      "source": [
        "from keras_vggface.vggface import VGGFace\n",
        "from keras_vggface.utils import preprocess_input\n",
        "from keras_vggface.utils import decode_predictions\n",
        "\n",
        "model = VGGFace(model='resnet50', include_top=False, input_shape=(224, 224, 3), pooling='avg')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W27tY7bvRje7"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PVVFyotxwKoi"
      },
      "source": [
        "from matplotlib import pyplot\n",
        "from PIL import Image\n",
        "from numpy import asarray\n",
        "from scipy.spatial.distance import cosine\n",
        "from mtcnn.mtcnn import MTCNN\n",
        "from keras_vggface.vggface import VGGFace\n",
        "from keras_vggface.utils import preprocess_input\n",
        " \n",
        "# extract a single face from a given photograph\n",
        "def extract_face(filename,size=(224,224)):\n",
        "  print(filename)\n",
        "  pixels= pyplot.imread(filename)\n",
        "  cv2_imshow(pixels)\n",
        "  detector = MTCNN()\n",
        "  results = detector.detect_faces(pixels) \n",
        "  x1, y1, width, height = results[0]['box']\n",
        "  x2, y2 = x1 + width, y1 + height\n",
        "  face = pixels[y1:y2, x1:x2]\n",
        "  cv2_imshow(cv2.resize(face,dsize=size))\n",
        "  image = Image.fromarray(face)\n",
        "  image = image.resize(size)\n",
        " \n",
        "  face_array = asarray(image)\n",
        "  return face_array\n",
        "\n",
        "# extract faces and calculate face embeddings for a list of photo files\n",
        "def get_embeddings(filenames):\n",
        "\t# extract faces\n",
        "\tfaces = [extract_face(f) for f in filenames]\n",
        "\t# convert into an array of samples\n",
        "\tsamples = asarray(faces, 'float32')\n",
        "\t# prepare the face for the model, e.g. center pixels\n",
        "\tsamples = preprocess_input(samples, version=2)\n",
        "\t# create a vggface model\n",
        "\tmodel = VGGFace(model='resnet50', include_top=False, input_shape=(224, 224, 3), pooling='avg')\n",
        "\t# perform prediction\n",
        "\tyhat = model.predict(samples)\n",
        "\treturn yhat\n",
        " \n",
        "# determine if a candidate face is a match for a known face\n",
        "def is_match(known_embedding, candidate_embedding, thresh=0.5):\n",
        "\t# calculate distance between embeddings\n",
        "\tscore = cosine(known_embedding, candidate_embedding)\n",
        "\tif score <= thresh:\n",
        "\t\tprint('>face is a Match (%.3f <= %.3f)' % (score, thresh))\n",
        "\telse:\n",
        "\t\tprint('>face is NOT a Match (%.3f > %.3f)' % (score, thresh))\n",
        " \n",
        "# define filenames\n",
        "filenames = ['/content/im_1.jfif', '/content/im_2.jpg',\n",
        "\t'/content/im-4.jfif', '/content/im_5.jfif']\n",
        "# get embeddings file filenames\n",
        "embeddings = get_embeddings(filenames)\n",
        "# define sharon stone\n",
        "sharon_id = embeddings[0]\n",
        "# verify known photos of sharon\n",
        "print('Positive Tests')\n",
        "is_match(embeddings[0], embeddings[1])\n",
        "is_match(embeddings[0], embeddings[2])\n",
        "# verify known photos of other people\n",
        "print('Negative Tests')\n",
        "is_match(embeddings[0], embeddings[3])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eIQ0NyAeG5ky"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h1K0upOgrERM"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pfmXtN8D08VV"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}